{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXu1r8qvSzWf"
   },
   "source": [
    "# Twin-Delayed DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TSHnuDthEfq",
    "outputId": "3efc1b82-271e-448c-d102-bd9943ca653c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "FOLDERNAME = 'CNN/archive'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_aM3IUIhE2J"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/Reinforcement-Learning/Code_Repository')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRzQUhuUTc0J"
   },
   "source": [
    "## Enviroment Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HAHMB0Ze8fU0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import Enviorment_Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmaekMHxg_cX",
    "outputId": "d87bf0a2-d8d1-4f4f-a417-77797b7b6b96",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../File_Repository/Train set/0050_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21396\\2248909282.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#env = Enviorment_Setting.ETFenv(id = \"0050\",reward_driver= 0.05, punish_driver = 0.05, length = 100, stock_num = 1000, interest_rate = 0.05)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"0050\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnviorment_Setting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mETFenv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward_driver\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpunish_driver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterest_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Oreo\\SALL_Agency\\RL\\Code_Repository\\Enviorment_Setting.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, id, reward_driver, punish_driver, length, stock_num, interest_rate)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0maddress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../File_Repository/Train set/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mid\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_train.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMACD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SALL_Agency\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SALL_Agency\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SALL_Agency\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SALL_Agency\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SALL_Agency\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SALL_Agency\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SALL_Agency\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SALL_Agency\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../File_Repository/Train set/0050_train.csv'"
     ]
    }
   ],
   "source": [
    "#env = Enviorment_Setting.ETFenv(id = \"0050\",reward_driver= 0.05, punish_driver = 0.05, length = 100, stock_num = 1000, interest_rate = 0.05)\n",
    "id = \"0050\"\n",
    "env = Enviorment_Setting.ETFenv(id = id,reward_driver= 0.05, punish_driver = 0.05, length = 100, stock_num = 1000, interest_rate = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8f0Gvg82eOE"
   },
   "outputs": [],
   "source": [
    "import Enviorment_Setting_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BpzxEuGk3_E7"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21396\\2865258311.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspaces\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDiscrete\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mspace_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcol_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'開盤價(元)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'最高價(元)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'最低價(元)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'收盤價(元)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'成交量(千股)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'成交值(千元)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'報酬率％'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'週轉率％'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'成交筆數(筆)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'合計買賣超(千股)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'融資餘額(張)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'融券餘額(張)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'MACD'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SALL_Agency\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gym import spaces\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "data = np.load(\"test.npy\")\n",
    "space_dict = {}\n",
    "col_array = ['開盤價(元)','最高價(元)','最低價(元)','收盤價(元)','成交量(千股)','成交值(千元)','報酬率％','週轉率％','成交筆數(筆)','合計買賣超(千股)','融資餘額(張)','融券餘額(張)','MACD']\n",
    "i=0\n",
    "for col in col_array:\n",
    "    space_dict[col] = spaces.Box(low=np.array([data[:,i].min()]), high=np.array([data[:,i].max()]),dtype=np.float64)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGMhX-Oq2eOE",
    "outputId": "7e8e517e-5452-4015-f51e-5d8aa499dec2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Reinforcement-Learning/Code_Repository/Enviorment_Setting_new.py:47: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.action_space = Box(low = -1, high= 1, shape=(1,), dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "#env = Enviorment_Setting.ETFenv(id = \"0050\",reward_driver= 0.05, punish_driver = 0.05, length = 100, stock_num = 1000, interest_rate = 0.05)\n",
    "\n",
    "env = Enviorment_Setting_new.ETFenv(data,space_dict,price_key='收盤價(元)',reward_driver= 0.05, punish_driver = 0.05, length = 100, stock_num = 1000, interest_rate = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGuKmH_ijf7U"
   },
   "source": [
    "## We set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFj6wbAo97lk"
   },
   "outputs": [],
   "source": [
    "env_name = \"SALL_ENV\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number 隨機種子\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network 隨機探索步數\n",
    "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps) 多少步後做一次評估\n",
    "max_timesteps = 30000 # Total number of iterations/timesteps 總共訓練步數\n",
    "save_models = False # Boolean checker whether or not to save the pre-trained model 儲存模型\n",
    "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise 探索的動作噪訊\n",
    "batch_size = 100 # Size of the batch 訓練批次量\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward 報酬遞減因子\n",
    "tau = 0.005 # Target network update rate 目標模型更新率\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes policy網路動作噪訊\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy ) 動作噪訊最大值\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated 多少 iteration 後，更新 policy 策略網路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xjm2onHdT-Av"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ikr2p0Js8iB4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2nGdtlKVydr"
   },
   "source": [
    "## Step 1: We initialize the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5rW0IDB8nTO",
    "outputId": "e27b1719-7b01-4d67-b261-4aaa82769fb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward, done = self.storage[i]\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "            batch_dones.append(np.array(done, copy=False))\n",
    "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb7TTaHxWbQD"
   },
   "source": [
    "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CeRW4D79HL0"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.layer_1(x)) # Relu\n",
    "        x = F.silu(self.layer_2(x)) # Relu\n",
    "        x = nn.Tanh()(self.layer_3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRDDce8FXef7"
   },
   "source": [
    "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCee7gwR9Jrs"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # Defining the first Critic neural network\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, 1)\n",
    "        # Defining the second Critic neural network\n",
    "        self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_5 = nn.Linear(400, 300)\n",
    "        self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        # Forward-Propagation on the first Critic Neural Network\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        # Forward-Propagation on the second Critic Neural Network\n",
    "        x2 = F.relu(self.layer_4(xu))\n",
    "        x2 = F.relu(self.layer_5(x2))\n",
    "        x2 = self.layer_6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzIDuONodenW"
   },
   "source": [
    "## Steps 4 to 15: Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzd0H1xukdKe"
   },
   "outputs": [],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "\n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "            state = torch.Tensor(batch_states).to(device)\n",
    "            next_state = torch.Tensor(batch_next_states).to(device)\n",
    "            action = torch.Tensor(batch_actions).to(device)\n",
    "            reward = torch.Tensor(batch_rewards).to(device)\n",
    "            done = torch.Tensor(batch_dones).to(device)\n",
    "\n",
    "            # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "            next_action = self.actor_target(next_state)\n",
    "\n",
    "            # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "\n",
    "            # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "\n",
    "            # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "\n",
    "            # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "            # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "            if it % policy_freq == 0:\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean() # self.actor(state) -> action actor_loss -> score of the action\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    # Making a save method to save a trained model\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka-ZRtQvjBex"
   },
   "source": [
    "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qabqiYdp9wDM"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    avg_reward = 0.\n",
    "    action_arr = []\n",
    "    for _ in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(obs))\n",
    "            action_arr.append(action)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "    avg_reward /= eval_episodes\n",
    "    print(\"%2f, \" %(float(sum(action_arr)/len(action_arr))))\n",
    "    print(\"\\n\")\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjwf2HCol3XP"
   },
   "source": [
    "## We create a file name for the two saved models: the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fyH8N5z-o3o",
    "outputId": "41580d48-dfa3-4258-e2f6-1955fe0f9a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_SALL_ENV(seed:0)\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s(seed:%s)\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kop-C96Aml8O"
   },
   "source": [
    "## We create a folder inside which will be saved the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Src07lvY-zXb"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../Model_Repository/results\"):\n",
    "    os.makedirs(\"../Model_Repository/results\")\n",
    "if save_models and not os.path.exists(\"../Model_Repository/pytorch_models\"):\n",
    "    os.makedirs(\"../Model_Repository/pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YdPG4HXnNsh"
   },
   "source": [
    "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3RufYec_ADj",
    "outputId": "28771dd7-c63d-47e5-91ae-7f56bc44540a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = len(env.observation_space)\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWEgDAQxnbem"
   },
   "source": [
    "## We create the policy network (the Actor model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTVvG7F8_EWg"
   },
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI60VN2Unklh"
   },
   "source": [
    "## We create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sd-ZsdXR_LgV"
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYOpCyiDnw7s"
   },
   "source": [
    "## We define a list where all the evaluation results over 10 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhC_5XJ__Orp",
    "outputId": "3a88d1e7-9487-44ca-da61-8cdeee916f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.036650, \n",
      "\n",
      "\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2.203340\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluations = [evaluate_policy(policy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31n5eb03p-Fm"
   },
   "source": [
    "## We initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vN5EvxK_QhT"
   },
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8QUNfnwg_cd"
   },
   "source": [
    "## Action Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJ1od8Lng_cd"
   },
   "outputs": [],
   "source": [
    "class Actions_Scale(gym.ActionWrapper):\n",
    "    def __init__(self, env, low_, high_):\n",
    "        super().__init__(env)\n",
    "        self.action_space = Box(low = low_, high= high_, shape=(1,), dtype=np.int)\n",
    "    def action(self,act):\n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9gsjvtPqLgT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_ouY4NH_Y0I",
    "outputId": "da99b2d6-e1a6-4cb2-e1be-ad1846290250",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 100 Episode Num: 1 Reward: 3.9992456556018876\n",
      "Total Timesteps: 200 Episode Num: 2 Reward: 0.7225533269157529\n",
      "Total Timesteps: 300 Episode Num: 3 Reward: 3.452117211297962\n",
      "Total Timesteps: 400 Episode Num: 4 Reward: 2.2898036088367495\n",
      "Total Timesteps: 500 Episode Num: 5 Reward: -0.9356141779412883\n",
      "Total Timesteps: 600 Episode Num: 6 Reward: 3.378509435131745\n",
      "Total Timesteps: 700 Episode Num: 7 Reward: -5913.545268095051\n",
      "Total Timesteps: 800 Episode Num: 8 Reward: 1.2837965121162025\n",
      "Total Timesteps: 900 Episode Num: 9 Reward: -5.705839223515319\n",
      "Total Timesteps: 1000 Episode Num: 10 Reward: 4.394196848415614\n",
      "Total Timesteps: 1100 Episode Num: 11 Reward: -4045.067297951578\n",
      "Total Timesteps: 1200 Episode Num: 12 Reward: -0.15266826417015472\n",
      "Total Timesteps: 1300 Episode Num: 13 Reward: 4.360255853507895\n",
      "Total Timesteps: 1400 Episode Num: 14 Reward: 3.5287835270626764\n",
      "Total Timesteps: 1500 Episode Num: 15 Reward: 1.5153805894050056\n",
      "Total Timesteps: 1600 Episode Num: 16 Reward: 5.731749838229415\n",
      "Total Timesteps: 1700 Episode Num: 17 Reward: -3.601501659601639\n",
      "Total Timesteps: 1800 Episode Num: 18 Reward: 2.8534158803190874\n",
      "Total Timesteps: 1900 Episode Num: 19 Reward: 2.990499741359045\n",
      "Total Timesteps: 2000 Episode Num: 20 Reward: 2.3460135893402883\n",
      "Total Timesteps: 2100 Episode Num: 21 Reward: 2.424072378694067\n",
      "Total Timesteps: 2200 Episode Num: 22 Reward: 1.859275930762176\n",
      "Total Timesteps: 2300 Episode Num: 23 Reward: -0.5089252654930854\n",
      "Total Timesteps: 2400 Episode Num: 24 Reward: 1.4405414292087584\n",
      "Total Timesteps: 2500 Episode Num: 25 Reward: 4.810238602974999\n",
      "Total Timesteps: 2600 Episode Num: 26 Reward: 6.770236543062253\n",
      "Total Timesteps: 2700 Episode Num: 27 Reward: 0.38124545721986836\n",
      "Total Timesteps: 2800 Episode Num: 28 Reward: 2.085668488138743\n",
      "Total Timesteps: 2900 Episode Num: 29 Reward: 3.1626636172487395\n",
      "Total Timesteps: 3000 Episode Num: 30 Reward: -0.10931977110488632\n",
      "Total Timesteps: 3100 Episode Num: 31 Reward: 1.811975385035573\n",
      "Total Timesteps: 3200 Episode Num: 32 Reward: -0.16837696257387696\n",
      "Total Timesteps: 3300 Episode Num: 33 Reward: 0.6815199130930351\n",
      "Total Timesteps: 3400 Episode Num: 34 Reward: 2.462076696611237\n",
      "Total Timesteps: 3500 Episode Num: 35 Reward: 3.2345841719529824\n",
      "Total Timesteps: 3600 Episode Num: 36 Reward: -2.5652174630896343\n",
      "Total Timesteps: 3700 Episode Num: 37 Reward: 2.4295434751468523\n",
      "Total Timesteps: 3800 Episode Num: 38 Reward: -1.836025794287683\n",
      "Total Timesteps: 3900 Episode Num: 39 Reward: 1.6149812546009321\n",
      "Total Timesteps: 4000 Episode Num: 40 Reward: 1.3098371691270358\n",
      "Total Timesteps: 4100 Episode Num: 41 Reward: 4.254911455696652\n",
      "Total Timesteps: 4200 Episode Num: 42 Reward: 3.9950280308340007\n",
      "Total Timesteps: 4300 Episode Num: 43 Reward: 2.40418135389966\n",
      "Total Timesteps: 4400 Episode Num: 44 Reward: -0.8491295831251646\n",
      "Total Timesteps: 4500 Episode Num: 45 Reward: -1.523329870509178\n",
      "Total Timesteps: 4600 Episode Num: 46 Reward: -10.78497582695903\n",
      "Total Timesteps: 4700 Episode Num: 47 Reward: 4.269600633785823\n",
      "Total Timesteps: 4800 Episode Num: 48 Reward: 3.6413626991615553\n",
      "Total Timesteps: 4900 Episode Num: 49 Reward: 0.3452071507847594\n",
      "Total Timesteps: 5000 Episode Num: 50 Reward: -2.442998788344087\n",
      "-1.000000, \n",
      "\n",
      "\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1.435160\n",
      "---------------------------------------\n",
      "Total Timesteps: 5100 Episode Num: 51 Reward: 8.773900934110417\n",
      "Total Timesteps: 5200 Episode Num: 52 Reward: 0.09659858339857647\n",
      "Total Timesteps: 5300 Episode Num: 53 Reward: -0.5534817069838102\n",
      "Total Timesteps: 5400 Episode Num: 54 Reward: 1.0158515023805523\n",
      "Total Timesteps: 5500 Episode Num: 55 Reward: 5.204056092544283\n",
      "Total Timesteps: 5600 Episode Num: 56 Reward: 3.4317084943402096\n",
      "Total Timesteps: 5700 Episode Num: 57 Reward: 4629.943275268506\n",
      "Total Timesteps: 5800 Episode Num: 58 Reward: -0.5431580890628737\n",
      "Total Timesteps: 5900 Episode Num: 59 Reward: 1.1686903165484284\n",
      "Total Timesteps: 6000 Episode Num: 60 Reward: 0.02550477353398674\n",
      "Total Timesteps: 6100 Episode Num: 61 Reward: -4.530322190288521\n",
      "Total Timesteps: 6200 Episode Num: 62 Reward: -3.700170263707015\n",
      "Total Timesteps: 6300 Episode Num: 63 Reward: -1.5069972990597857\n",
      "Total Timesteps: 6400 Episode Num: 64 Reward: 3.5450191020296793\n",
      "Total Timesteps: 6500 Episode Num: 65 Reward: 3.863440236972165\n",
      "Total Timesteps: 6600 Episode Num: 66 Reward: -2.2955538512241485\n",
      "Total Timesteps: 6700 Episode Num: 67 Reward: 6.15912252970455\n",
      "Total Timesteps: 6800 Episode Num: 68 Reward: 2.8626937690511127\n",
      "Total Timesteps: 6900 Episode Num: 69 Reward: 3.13403713771093\n",
      "Total Timesteps: 7000 Episode Num: 70 Reward: -4576.132776734612\n",
      "Total Timesteps: 7100 Episode Num: 71 Reward: 3.9453008320644014\n",
      "Total Timesteps: 7200 Episode Num: 72 Reward: 1.1052985914534577\n",
      "Total Timesteps: 7300 Episode Num: 73 Reward: 3.0686128562064896\n",
      "Total Timesteps: 7400 Episode Num: 74 Reward: 4.021733290447563\n",
      "Total Timesteps: 7500 Episode Num: 75 Reward: 4.305204754108205\n",
      "Total Timesteps: 7600 Episode Num: 76 Reward: -1.3363498080842537\n",
      "Total Timesteps: 7700 Episode Num: 77 Reward: 5.664010898915025\n",
      "Total Timesteps: 7800 Episode Num: 78 Reward: 4.172233409285231\n",
      "Total Timesteps: 7900 Episode Num: 79 Reward: 3.1217524520051456\n",
      "Total Timesteps: 8000 Episode Num: 80 Reward: -1.0815669245044595\n",
      "Total Timesteps: 8100 Episode Num: 81 Reward: -1.1733487574489376\n",
      "Total Timesteps: 8200 Episode Num: 82 Reward: 0.9754651717575573\n",
      "Total Timesteps: 8300 Episode Num: 83 Reward: -0.5317886572711817\n",
      "Total Timesteps: 8400 Episode Num: 84 Reward: 0.3437761216288443\n",
      "Total Timesteps: 8500 Episode Num: 85 Reward: 2.103239096918699\n",
      "Total Timesteps: 8600 Episode Num: 86 Reward: 0.8969109908437969\n",
      "Total Timesteps: 8700 Episode Num: 87 Reward: 1.8116902663810484\n",
      "Total Timesteps: 8800 Episode Num: 88 Reward: 0.17673442377000353\n",
      "Total Timesteps: 8900 Episode Num: 89 Reward: 0.11158130061889496\n",
      "Total Timesteps: 9000 Episode Num: 90 Reward: 1.3525994877012921\n",
      "Total Timesteps: 9100 Episode Num: 91 Reward: -0.28218510213826264\n",
      "Total Timesteps: 9200 Episode Num: 92 Reward: -0.391353061690103\n",
      "Total Timesteps: 9300 Episode Num: 93 Reward: 0.8623675327574792\n",
      "Total Timesteps: 9400 Episode Num: 94 Reward: -0.06552883337605783\n",
      "Total Timesteps: 9500 Episode Num: 95 Reward: -2.0092802189945744\n",
      "Total Timesteps: 9600 Episode Num: 96 Reward: -2.9218698661044296\n",
      "Total Timesteps: 9700 Episode Num: 97 Reward: 0.9622986260267384\n",
      "Total Timesteps: 9800 Episode Num: 98 Reward: -8.769513915894981\n",
      "Total Timesteps: 9900 Episode Num: 99 Reward: -0.18460298244312287\n",
      "Total Timesteps: 10000 Episode Num: 100 Reward: 3.654723046917146\n",
      "-1.000000, \n",
      "\n",
      "\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -0.213561\n",
      "---------------------------------------\n",
      "Total Timesteps: 10100 Episode Num: 101 Reward: -1.0884471835877687\n",
      "Total Timesteps: 10200 Episode Num: 102 Reward: -1.200327069960197\n",
      "Total Timesteps: 10300 Episode Num: 103 Reward: 2.2081304217496185\n",
      "Total Timesteps: 10400 Episode Num: 104 Reward: -4.221054372037894\n",
      "Total Timesteps: 10500 Episode Num: 105 Reward: -3.740621759843839\n",
      "Total Timesteps: 10600 Episode Num: 106 Reward: -5.8669695475145\n",
      "Total Timesteps: 10700 Episode Num: 107 Reward: 27.739847204079112\n",
      "Total Timesteps: 10800 Episode Num: 108 Reward: -5.383594233112073\n",
      "Total Timesteps: 10900 Episode Num: 109 Reward: 0.03856336122449311\n",
      "Total Timesteps: 11000 Episode Num: 110 Reward: 46.12807991963733\n",
      "Total Timesteps: 11100 Episode Num: 111 Reward: -1.1076955610977235\n",
      "Total Timesteps: 11200 Episode Num: 112 Reward: 14.977174428624975\n",
      "Total Timesteps: 11300 Episode Num: 113 Reward: -2.7605834746359257\n",
      "Total Timesteps: 11400 Episode Num: 114 Reward: -2.0421027775585205\n",
      "Total Timesteps: 11500 Episode Num: 115 Reward: -0.2725907901861775\n",
      "Total Timesteps: 11600 Episode Num: 116 Reward: -13.23481340118248\n",
      "Total Timesteps: 11700 Episode Num: 117 Reward: -5.945651986177946\n",
      "Total Timesteps: 11800 Episode Num: 118 Reward: -12.748443427935898\n",
      "Total Timesteps: 11900 Episode Num: 119 Reward: -3.411759407867496\n",
      "Total Timesteps: 12000 Episode Num: 120 Reward: -7.488505311660815\n",
      "Total Timesteps: 12100 Episode Num: 121 Reward: -19.508786921612444\n",
      "Total Timesteps: 12200 Episode Num: 122 Reward: -0.25518050561809935\n",
      "Total Timesteps: 12300 Episode Num: 123 Reward: -1.96760357162479\n",
      "Total Timesteps: 12400 Episode Num: 124 Reward: -0.23288382987683098\n",
      "Total Timesteps: 12500 Episode Num: 125 Reward: -2.703842813908963\n",
      "Total Timesteps: 12600 Episode Num: 126 Reward: 5.943726777759242\n",
      "Total Timesteps: 12700 Episode Num: 127 Reward: 5.381361565341336\n",
      "Total Timesteps: 12800 Episode Num: 128 Reward: -1.100763438602701\n",
      "Total Timesteps: 12900 Episode Num: 129 Reward: -3.6494025358762676\n",
      "Total Timesteps: 13000 Episode Num: 130 Reward: 4.595351878658992\n",
      "Total Timesteps: 13100 Episode Num: 131 Reward: -1.0398282354052477\n",
      "Total Timesteps: 13200 Episode Num: 132 Reward: -4.261211002774137\n",
      "Total Timesteps: 13300 Episode Num: 133 Reward: -1.072829925545015\n",
      "Total Timesteps: 13400 Episode Num: 134 Reward: -0.9724319039396433\n",
      "Total Timesteps: 13500 Episode Num: 135 Reward: 43.65736055344853\n",
      "Total Timesteps: 13600 Episode Num: 136 Reward: -4.04233821117749\n",
      "Total Timesteps: 13700 Episode Num: 137 Reward: -10.746347889438752\n",
      "Total Timesteps: 13800 Episode Num: 138 Reward: -4.956690062904659\n",
      "Total Timesteps: 13900 Episode Num: 139 Reward: -4.741281208002068\n",
      "Total Timesteps: 14000 Episode Num: 140 Reward: -4.791388483360027\n",
      "Total Timesteps: 14100 Episode Num: 141 Reward: -1.7646156004751752\n",
      "Total Timesteps: 14200 Episode Num: 142 Reward: -2.915132127252224\n",
      "Total Timesteps: 14300 Episode Num: 143 Reward: 31.69862124281089\n",
      "Total Timesteps: 14400 Episode Num: 144 Reward: 6.003814177216782\n",
      "Total Timesteps: 14500 Episode Num: 145 Reward: -5.045351516354211\n",
      "Total Timesteps: 14600 Episode Num: 146 Reward: -1.1446777953462313\n",
      "Total Timesteps: 14700 Episode Num: 147 Reward: -2.2864281764232333\n",
      "Total Timesteps: 14800 Episode Num: 148 Reward: -6.88767065718124\n",
      "Total Timesteps: 14900 Episode Num: 149 Reward: 0.5230424663040596\n",
      "Total Timesteps: 15000 Episode Num: 150 Reward: 0.8219297757283636\n",
      "-1.000000, \n",
      "\n",
      "\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -5.692327\n",
      "---------------------------------------\n",
      "Total Timesteps: 15100 Episode Num: 151 Reward: -1.3164597756300518\n",
      "Total Timesteps: 15200 Episode Num: 152 Reward: -3.555246417545654\n",
      "Total Timesteps: 15300 Episode Num: 153 Reward: -0.629066053089442\n",
      "Total Timesteps: 15400 Episode Num: 154 Reward: 1.5684337904487757\n",
      "Total Timesteps: 15500 Episode Num: 155 Reward: -0.2285700448767185\n",
      "Total Timesteps: 15600 Episode Num: 156 Reward: 0.8147649143527679\n",
      "Total Timesteps: 15700 Episode Num: 157 Reward: -6.160013043649183\n",
      "Total Timesteps: 15800 Episode Num: 158 Reward: 4.307994070415944\n",
      "Total Timesteps: 15900 Episode Num: 159 Reward: -5.740246081784019\n",
      "Total Timesteps: 16000 Episode Num: 160 Reward: -1.9781107708873382\n",
      "Total Timesteps: 16100 Episode Num: 161 Reward: -2.610831688532802\n",
      "Total Timesteps: 16200 Episode Num: 162 Reward: 5.094040261494056\n",
      "Total Timesteps: 16300 Episode Num: 163 Reward: 5.448594858673803\n",
      "Total Timesteps: 16400 Episode Num: 164 Reward: 0.7322908153356291\n",
      "Total Timesteps: 16500 Episode Num: 165 Reward: -5.375347431314174\n",
      "Total Timesteps: 16600 Episode Num: 166 Reward: 1.3655121497406668\n",
      "Total Timesteps: 16700 Episode Num: 167 Reward: -3.1341095696061623\n",
      "Total Timesteps: 16800 Episode Num: 168 Reward: 0.13182270992915557\n",
      "Total Timesteps: 16900 Episode Num: 169 Reward: 5.184990527186047\n",
      "Total Timesteps: 17000 Episode Num: 170 Reward: -2.9040666784012092\n",
      "Total Timesteps: 17100 Episode Num: 171 Reward: -3.4603481922921757\n",
      "Total Timesteps: 17200 Episode Num: 172 Reward: 1.3114533539014666\n",
      "Total Timesteps: 17300 Episode Num: 173 Reward: -9.634854921210746\n",
      "Total Timesteps: 17400 Episode Num: 174 Reward: 6.748422015552252\n",
      "Total Timesteps: 17500 Episode Num: 175 Reward: -7.742538456361766\n",
      "Total Timesteps: 17600 Episode Num: 176 Reward: -3.8014993320068635\n",
      "Total Timesteps: 17700 Episode Num: 177 Reward: 10.928937612398745\n",
      "Total Timesteps: 17800 Episode Num: 178 Reward: -3.9060280526603774\n",
      "Total Timesteps: 17900 Episode Num: 179 Reward: -2.893145521761941\n",
      "Total Timesteps: 18000 Episode Num: 180 Reward: -0.7885648205550926\n",
      "Total Timesteps: 18100 Episode Num: 181 Reward: -3.857033971227562\n",
      "Total Timesteps: 18200 Episode Num: 182 Reward: -4.125309841700968\n",
      "Total Timesteps: 18300 Episode Num: 183 Reward: -3.2571591183706587\n",
      "Total Timesteps: 18400 Episode Num: 184 Reward: -3.4568473806197066\n",
      "Total Timesteps: 18500 Episode Num: 185 Reward: -5.647033119427185\n",
      "Total Timesteps: 18600 Episode Num: 186 Reward: 420.3550089600891\n",
      "Total Timesteps: 18700 Episode Num: 187 Reward: -1.9132789236718428\n",
      "Total Timesteps: 18800 Episode Num: 188 Reward: 1.8331090194784088\n",
      "Total Timesteps: 18900 Episode Num: 189 Reward: 8.721021076325245\n",
      "Total Timesteps: 19000 Episode Num: 190 Reward: -9.448672025749925\n",
      "Total Timesteps: 19100 Episode Num: 191 Reward: -3.7105380738855405\n",
      "Total Timesteps: 19200 Episode Num: 192 Reward: 0.04189716526106818\n",
      "Total Timesteps: 19300 Episode Num: 193 Reward: 2.9910307840220693\n",
      "Total Timesteps: 19400 Episode Num: 194 Reward: -0.7281675164173321\n",
      "Total Timesteps: 19500 Episode Num: 195 Reward: -1.5058850386805744\n",
      "Total Timesteps: 19600 Episode Num: 196 Reward: 0.662490740014451\n",
      "Total Timesteps: 19700 Episode Num: 197 Reward: 4.711163553076579\n",
      "Total Timesteps: 19800 Episode Num: 198 Reward: -3.2728474924431\n",
      "Total Timesteps: 19900 Episode Num: 199 Reward: -13.225827822309256\n",
      "Total Timesteps: 20000 Episode Num: 200 Reward: -0.2987002903816507\n",
      "-1.000000, \n",
      "\n",
      "\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1.711146\n",
      "---------------------------------------\n",
      "Total Timesteps: 20100 Episode Num: 201 Reward: 7.802178048982896\n",
      "Total Timesteps: 20200 Episode Num: 202 Reward: -35.559706177744005\n",
      "Total Timesteps: 20300 Episode Num: 203 Reward: 0.4728277614366442\n",
      "Total Timesteps: 20400 Episode Num: 204 Reward: -1.1906020954128265\n",
      "Total Timesteps: 20500 Episode Num: 205 Reward: 4.354147477610074\n",
      "Total Timesteps: 20600 Episode Num: 206 Reward: -3.479436794571204\n",
      "Total Timesteps: 20700 Episode Num: 207 Reward: 20.176268987267274\n",
      "Total Timesteps: 20800 Episode Num: 208 Reward: -2.0419340467917935\n",
      "Total Timesteps: 20900 Episode Num: 209 Reward: -3.8312275565225513\n",
      "Total Timesteps: 21000 Episode Num: 210 Reward: 0.47356812544066723\n",
      "Total Timesteps: 21100 Episode Num: 211 Reward: 0.41170401287300157\n",
      "Total Timesteps: 21200 Episode Num: 212 Reward: 0.5527103213607543\n",
      "Total Timesteps: 21300 Episode Num: 213 Reward: 7.74796105141669\n",
      "Total Timesteps: 21400 Episode Num: 214 Reward: 5.184320686585075\n",
      "Total Timesteps: 21500 Episode Num: 215 Reward: -3.130035401607363\n",
      "Total Timesteps: 21600 Episode Num: 216 Reward: -0.5543329541087876\n",
      "Total Timesteps: 21700 Episode Num: 217 Reward: 0.31336681779774217\n",
      "Total Timesteps: 21800 Episode Num: 218 Reward: 10.083026257108777\n",
      "Total Timesteps: 21900 Episode Num: 219 Reward: -0.3884371891902948\n",
      "Total Timesteps: 22000 Episode Num: 220 Reward: -2.3968457044928693\n",
      "Total Timesteps: 22100 Episode Num: 221 Reward: 7.157473782349762\n",
      "Total Timesteps: 22200 Episode Num: 222 Reward: -4.470446895834096\n",
      "Total Timesteps: 22300 Episode Num: 223 Reward: 7.716760674873309\n",
      "Total Timesteps: 22400 Episode Num: 224 Reward: -4.18735295305352\n",
      "Total Timesteps: 22500 Episode Num: 225 Reward: -14.073236508070076\n",
      "Total Timesteps: 22600 Episode Num: 226 Reward: 7.240302134821153\n",
      "Total Timesteps: 22700 Episode Num: 227 Reward: -0.22235952002238635\n",
      "Total Timesteps: 22800 Episode Num: 228 Reward: 5.696098476699747\n",
      "Total Timesteps: 22900 Episode Num: 229 Reward: -3.3142032382308484\n",
      "Total Timesteps: 23000 Episode Num: 230 Reward: 3.981293625463695\n",
      "Total Timesteps: 23100 Episode Num: 231 Reward: -3.5463796197770057\n",
      "Total Timesteps: 23200 Episode Num: 232 Reward: -2.5397992828146125\n",
      "Total Timesteps: 23300 Episode Num: 233 Reward: -2.802344477705599\n",
      "Total Timesteps: 23400 Episode Num: 234 Reward: -4.414071413424696\n",
      "Total Timesteps: 23500 Episode Num: 235 Reward: 3.026369534406088\n",
      "Total Timesteps: 23600 Episode Num: 236 Reward: -3.7297853562137337\n",
      "Total Timesteps: 23700 Episode Num: 237 Reward: 6.162868747219374\n",
      "Total Timesteps: 23800 Episode Num: 238 Reward: -0.30765173990973405\n",
      "Total Timesteps: 23900 Episode Num: 239 Reward: 1.9303852341380876\n",
      "Total Timesteps: 24000 Episode Num: 240 Reward: -3.6826771898338024\n",
      "Total Timesteps: 24100 Episode Num: 241 Reward: 6.70943375832705\n",
      "Total Timesteps: 24200 Episode Num: 242 Reward: -0.2703406493147925\n",
      "Total Timesteps: 24300 Episode Num: 243 Reward: 3.556406967783089\n",
      "Total Timesteps: 24400 Episode Num: 244 Reward: 3.90497877660014\n",
      "Total Timesteps: 24500 Episode Num: 245 Reward: 4.448286493344835\n",
      "Total Timesteps: 24600 Episode Num: 246 Reward: 485.66524554412103\n",
      "Total Timesteps: 24700 Episode Num: 247 Reward: 2.9834707033900703\n",
      "Total Timesteps: 24800 Episode Num: 248 Reward: -0.9040837600268432\n",
      "Total Timesteps: 24900 Episode Num: 249 Reward: 0.8118665490488214\n",
      "Total Timesteps: 25000 Episode Num: 250 Reward: -5.955162029382991\n",
      "-1.000000, \n",
      "\n",
      "\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1.041342\n",
      "---------------------------------------\n",
      "Total Timesteps: 25100 Episode Num: 251 Reward: -0.7831115775697945\n",
      "Total Timesteps: 25200 Episode Num: 252 Reward: 1.3833352421196603\n",
      "Total Timesteps: 25300 Episode Num: 253 Reward: -0.3416967554651815\n",
      "Total Timesteps: 25400 Episode Num: 254 Reward: 1.2131997387516569\n",
      "Total Timesteps: 25500 Episode Num: 255 Reward: 0.12653769474861432\n",
      "Total Timesteps: 25600 Episode Num: 256 Reward: -4.0834927307139335\n",
      "Total Timesteps: 25700 Episode Num: 257 Reward: 3.3038868743301326\n",
      "Total Timesteps: 25800 Episode Num: 258 Reward: -3.5534739723533577\n",
      "Total Timesteps: 25900 Episode Num: 259 Reward: 0.9230856997164616\n",
      "Total Timesteps: 26000 Episode Num: 260 Reward: 2.630302848447353\n",
      "Total Timesteps: 26100 Episode Num: 261 Reward: -0.4805920641607079\n",
      "Total Timesteps: 26200 Episode Num: 262 Reward: 6.440886108688333\n",
      "Total Timesteps: 26300 Episode Num: 263 Reward: 2.8583528025526967\n",
      "Total Timesteps: 26400 Episode Num: 264 Reward: 3.055239590377913\n",
      "Total Timesteps: 26500 Episode Num: 265 Reward: 5.590915704938232\n",
      "Total Timesteps: 26600 Episode Num: 266 Reward: 2.7610478178466216\n",
      "Total Timesteps: 26700 Episode Num: 267 Reward: 0.5028879772129067\n",
      "Total Timesteps: 26800 Episode Num: 268 Reward: -116.16600983527405\n",
      "Total Timesteps: 26900 Episode Num: 269 Reward: 7.1848811964693\n",
      "Total Timesteps: 27000 Episode Num: 270 Reward: -11.881177966645767\n",
      "Total Timesteps: 27100 Episode Num: 271 Reward: 1.2247663758189422\n",
      "Total Timesteps: 27200 Episode Num: 272 Reward: -9.305799089999176\n",
      "Total Timesteps: 27300 Episode Num: 273 Reward: -0.4789503486033037\n",
      "Total Timesteps: 27400 Episode Num: 274 Reward: -4.49352537457549\n",
      "Total Timesteps: 27500 Episode Num: 275 Reward: -6.303374393687968\n",
      "Total Timesteps: 27600 Episode Num: 276 Reward: 9.092764509267397\n",
      "Total Timesteps: 27700 Episode Num: 277 Reward: -0.2990694064160668\n",
      "Total Timesteps: 27800 Episode Num: 278 Reward: -18.341435164224777\n",
      "Total Timesteps: 27900 Episode Num: 279 Reward: -2.8263784661071183\n",
      "Total Timesteps: 28000 Episode Num: 280 Reward: 2.235895236086117\n",
      "Total Timesteps: 28100 Episode Num: 281 Reward: 4.424106767034185\n",
      "Total Timesteps: 28200 Episode Num: 282 Reward: -2.241049393731073\n",
      "Total Timesteps: 28300 Episode Num: 283 Reward: 7.198014820080397\n",
      "Total Timesteps: 28400 Episode Num: 284 Reward: -17.64407087922155\n",
      "Total Timesteps: 28500 Episode Num: 285 Reward: 0.5487233795852368\n",
      "Total Timesteps: 28600 Episode Num: 286 Reward: 2.5006463759815345\n",
      "Total Timesteps: 28700 Episode Num: 287 Reward: -3.916410888944668\n",
      "Total Timesteps: 28800 Episode Num: 288 Reward: 7.903139517617321\n",
      "Total Timesteps: 28900 Episode Num: 289 Reward: 0.7106895192802272\n",
      "Total Timesteps: 29000 Episode Num: 290 Reward: -2.881960736075838\n",
      "Total Timesteps: 29100 Episode Num: 291 Reward: 6.489019525258357\n",
      "Total Timesteps: 29200 Episode Num: 292 Reward: 537.014434822313\n",
      "Total Timesteps: 29300 Episode Num: 293 Reward: 1.8107994702491195\n",
      "Total Timesteps: 29400 Episode Num: 294 Reward: -6.339674095576346\n",
      "Total Timesteps: 29500 Episode Num: 295 Reward: -0.11079429272131681\n",
      "Total Timesteps: 29600 Episode Num: 296 Reward: -0.13155546542347613\n",
      "Total Timesteps: 29700 Episode Num: 297 Reward: -2.883331034711178\n",
      "Total Timesteps: 29800 Episode Num: 298 Reward: 0.26911171964614433\n",
      "Total Timesteps: 29900 Episode Num: 299 Reward: -1.3889259313333644\n",
      "-1.000000, \n",
      "\n",
      "\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 0.966980\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We start the main loop over 500,000 timesteps\n",
    "max_episode_steps = env._max_episode_steps\n",
    "env_og = env\n",
    "while total_timesteps < max_timesteps:\n",
    "\n",
    "    # If the episode is done\n",
    "    if done:\n",
    "\n",
    "        # If we are not at the very beginning, we start the training process of the model\n",
    "        if total_timesteps != 0:\n",
    "            print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "            policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "        # We evaluate the episode and we save the policy\n",
    "        if timesteps_since_eval >= eval_freq:\n",
    "            timesteps_since_eval %= eval_freq\n",
    "            evaluations.append(evaluate_policy(policy))\n",
    "            policy.save(file_name, directory=\"../Model_Repository/pytorch_models\")\n",
    "            np.save(\"../Model_Repository/results/%s\" % (file_name), evaluations)\n",
    "\n",
    "        # When the training step is done, we reset the state of the environment\n",
    "        obs = env.reset()\n",
    "\n",
    "        # Set the Done to False\n",
    "        done = False\n",
    "\n",
    "        # Set rewards and episode timesteps to zero\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1\n",
    "\n",
    "    # Before 10000 timesteps, we play random actions\n",
    "    if total_timesteps < start_timesteps:\n",
    "        action = env.action_space.sample()\n",
    "    else: # After 10000 timesteps, we switch to the model\n",
    "        # 這邊加 wrapper autoscale action\n",
    "        action = policy.select_action(np.array(obs))\n",
    "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "        if expl_noise != 0:\n",
    "            action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "    # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "    new_obs, reward, done, _ = env.step(action)\n",
    "    # We check if the episode is done\n",
    "    done_bool = 0 if episode_timesteps + 1 == max_episode_steps else float(done)\n",
    "\n",
    "    # We increase the total reward\n",
    "    episode_reward += reward\n",
    "\n",
    "    # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "    replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "    # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "    obs = new_obs\n",
    "    episode_timesteps += 1\n",
    "    total_timesteps += 1\n",
    "    timesteps_since_eval += 1\n",
    "\n",
    "# We add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"../Model_Repository/pytorch_models\")\n",
    "np.save(\"../Model_Repository/results/%s\" % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErS13vajyq7v",
    "outputId": "60e53270-c0d2-450b-d6f5-c0085d967041"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5286252256739113, array([1.]), 0.4756302573445379, 0.4873949631092436)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward,action,env.price,env.next_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCCaxTKkg_cf"
   },
   "source": [
    "## PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIQk-GFsg_cf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "test_address = \"../File_Repository/Test set/\" + id + \"_test.csv\"\n",
    "train_address = \"../File_Repository/Train set/\" + id + \"_train.csv\"\n",
    "test_dataset = pd.read_csv(test_address)\n",
    "train_dataset = pd.read_csv(train_address)\n",
    "\n",
    "X_train = train_dataset.iloc[:, 1:].values\n",
    "X_test = test_dataset.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMreQqaRg_cf"
   },
   "outputs": [],
   "source": [
    "def calculate_ema(prices, days, smoothing=2):\n",
    "    ema = [sum(prices[:days]) / days]\n",
    "    for price in prices[days:]:\n",
    "        ema.append((price * (smoothing / (1 + days))) + ema[-1] * (1 - (smoothing / (1 + days))))\n",
    "    return ema\n",
    "\n",
    "def MACD(list_input):\n",
    "    prices = list_input[:,0]\n",
    "    ema_short = calculate_ema(prices, 12)\n",
    "    ema_long = calculate_ema(prices, 26)\n",
    "    len(ema_short)\n",
    "    dif = []\n",
    "    for i in range(0,len(prices)-len(ema_short)):\n",
    "        tmp = ema_short[i]\n",
    "        ema_short.insert( i, tmp)\n",
    "\n",
    "    for j in range(0,len(prices)-len(ema_long)):\n",
    "        tmp = ema_long[i]\n",
    "        ema_long.insert( i, tmp)\n",
    "\n",
    "    for k in range(0,len(prices)):\n",
    "        dif.append(ema_short[k] - ema_long[k])\n",
    "\n",
    "    MACD = calculate_ema(dif, 9)\n",
    "    for j in range(0,len(prices)-len(MACD)):\n",
    "        tmp = MACD[i]\n",
    "        MACD.insert( i, tmp)\n",
    "\n",
    "    MACD_arr = np.array(MACD).reshape(len(MACD),1)\n",
    "    list_input = np.hstack((list_input,MACD_arr))\n",
    "    return list_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfM3--xjYPtM",
    "outputId": "ebed5641-f1c6-414e-d2c2-f62a5f46898b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bA94xzahg_cf"
   },
   "outputs": [],
   "source": [
    "X_train = MACD(list_input = X_train)\n",
    "X_test = MACD(list_input = X_test)\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0.00000001, 1))\n",
    "scaler.fit(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNN7UVm-6yER"
   },
   "outputs": [],
   "source": [
    "X_test = data[900:1200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "HxAP2AXWg_cf",
    "outputId": "3b29dd93-6ad2-4623-87a5-d0543680a65b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "action_list = []\n",
    "price_list = []\n",
    "n_list = []\n",
    "reward_list = []\n",
    "index = 0\n",
    "for n in X_test:\n",
    "    action = policy.select_action(np.array(n))\n",
    "    action_list.append(action)\n",
    "    price_list.append(n[0])\n",
    "    index +=1\n",
    "    n_list.append(index)\n",
    "\n",
    "price_list.pop(0)\n",
    "n_list.pop(0)\n",
    "action_list.pop()\n",
    "# 製作figure\n",
    "fig = plt.figure()\n",
    "\n",
    "#圖表的設定\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "#直線圖\n",
    "ax.plot(n_list,price_list, color='grey',linewidth=0.5,label='price trend')\n",
    "#散佈圖\n",
    "norm = plt.Normalize(-1, 1)\n",
    "ax.scatter(n_list, price_list, c=norm(action_list), cmap='coolwarm',s=3);\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 紅色為正值 藍色為負值\n",
    "# print('本次測試結果，將會賺得：' + str(money-100000) + '元(本金為100000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jvtwnv25g_cg",
    "outputId": "ae44c8fb-062a-437d-c60b-a98f737761d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.99999547], dtype=float32),\n",
       " array([0.9999986], dtype=float32),\n",
       " array([0.99999946], dtype=float32),\n",
       " array([0.99999994], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([0.9999999], dtype=float32),\n",
       " array([0.99999994], dtype=float32),\n",
       " array([-0.99998224], dtype=float32),\n",
       " array([0.9998285], dtype=float32),\n",
       " array([0.99999654], dtype=float32),\n",
       " array([0.9995096], dtype=float32),\n",
       " array([0.9998627], dtype=float32),\n",
       " array([0.99992377], dtype=float32),\n",
       " array([0.9999926], dtype=float32),\n",
       " array([0.9999958], dtype=float32),\n",
       " array([0.9991361], dtype=float32),\n",
       " array([0.99987394], dtype=float32),\n",
       " array([0.99997216], dtype=float32),\n",
       " array([0.9999983], dtype=float32),\n",
       " array([0.9999823], dtype=float32),\n",
       " array([0.99998945], dtype=float32),\n",
       " array([0.99998707], dtype=float32),\n",
       " array([0.99998385], dtype=float32),\n",
       " array([0.9997745], dtype=float32),\n",
       " array([0.99983054], dtype=float32),\n",
       " array([0.9895123], dtype=float32),\n",
       " array([0.99873286], dtype=float32),\n",
       " array([0.99916065], dtype=float32),\n",
       " array([0.9996867], dtype=float32),\n",
       " array([0.99979556], dtype=float32),\n",
       " array([0.99940675], dtype=float32),\n",
       " array([0.99968326], dtype=float32),\n",
       " array([0.99796057], dtype=float32),\n",
       " array([0.89297795], dtype=float32),\n",
       " array([0.99368256], dtype=float32),\n",
       " array([0.9898224], dtype=float32),\n",
       " array([0.9696698], dtype=float32),\n",
       " array([0.13608548], dtype=float32),\n",
       " array([0.7999083], dtype=float32),\n",
       " array([0.92736286], dtype=float32),\n",
       " array([0.98367816], dtype=float32),\n",
       " array([0.97541577], dtype=float32),\n",
       " array([0.9591869], dtype=float32),\n",
       " array([-0.9992343], dtype=float32),\n",
       " array([-0.9754896], dtype=float32),\n",
       " array([0.2715312], dtype=float32),\n",
       " array([-0.99999976], dtype=float32),\n",
       " array([-0.66634846], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-0.99935675], dtype=float32),\n",
       " array([-0.99113065], dtype=float32),\n",
       " array([-0.9999683], dtype=float32),\n",
       " array([-0.99999994], dtype=float32),\n",
       " array([-0.9996477], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-0.31536344], dtype=float32),\n",
       " array([-0.27707326], dtype=float32),\n",
       " array([-0.10140812], dtype=float32),\n",
       " array([0.1464743], dtype=float32),\n",
       " array([-0.99767387], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([0.9999754], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([0.99682015], dtype=float32),\n",
       " array([0.99885976], dtype=float32),\n",
       " array([0.99990565], dtype=float32),\n",
       " array([0.9998774], dtype=float32),\n",
       " array([0.9999999], dtype=float32),\n",
       " array([0.9992849], dtype=float32),\n",
       " array([0.9975248], dtype=float32),\n",
       " array([0.99803555], dtype=float32),\n",
       " array([0.99986744], dtype=float32),\n",
       " array([0.9993873], dtype=float32),\n",
       " array([0.9990321], dtype=float32),\n",
       " array([0.9994879], dtype=float32),\n",
       " array([-0.22356795], dtype=float32),\n",
       " array([0.87704414], dtype=float32),\n",
       " array([0.7013899], dtype=float32),\n",
       " array([0.494954], dtype=float32),\n",
       " array([0.9842499], dtype=float32),\n",
       " array([0.9862659], dtype=float32),\n",
       " array([0.60186476], dtype=float32),\n",
       " array([0.568745], dtype=float32),\n",
       " array([0.6817111], dtype=float32),\n",
       " array([0.79865575], dtype=float32),\n",
       " array([-0.46263838], dtype=float32),\n",
       " array([0.981313], dtype=float32),\n",
       " array([-0.99947906], dtype=float32),\n",
       " array([-0.9906446], dtype=float32),\n",
       " array([0.8152542], dtype=float32),\n",
       " array([0.95113355], dtype=float32),\n",
       " array([0.88931036], dtype=float32),\n",
       " array([-0.99996084], dtype=float32),\n",
       " array([0.93664026], dtype=float32),\n",
       " array([0.7115281], dtype=float32),\n",
       " array([-0.14202325], dtype=float32),\n",
       " array([0.9564743], dtype=float32),\n",
       " array([0.89844], dtype=float32),\n",
       " array([0.9504286], dtype=float32),\n",
       " array([0.5604048], dtype=float32),\n",
       " array([-0.9960712], dtype=float32),\n",
       " array([-0.5811873], dtype=float32),\n",
       " array([-0.9998813], dtype=float32),\n",
       " array([-0.9999746], dtype=float32),\n",
       " array([-0.9999996], dtype=float32),\n",
       " array([-0.99999774], dtype=float32),\n",
       " array([-0.724558], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([0.67006934], dtype=float32),\n",
       " array([0.99170667], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([0.99793154], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([0.9048052], dtype=float32),\n",
       " array([0.98242724], dtype=float32),\n",
       " array([0.92750156], dtype=float32),\n",
       " array([0.9916272], dtype=float32),\n",
       " array([0.96943176], dtype=float32),\n",
       " array([0.9942401], dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([0.99956346], dtype=float32),\n",
       " array([0.98391086], dtype=float32),\n",
       " array([0.86959326], dtype=float32),\n",
       " array([-0.21712475], dtype=float32),\n",
       " array([0.952685], dtype=float32),\n",
       " array([0.9873036], dtype=float32),\n",
       " array([0.95055676], dtype=float32),\n",
       " array([-0.9971584], dtype=float32),\n",
       " array([-0.6808522], dtype=float32),\n",
       " array([0.33227748], dtype=float32),\n",
       " array([0.3423542], dtype=float32),\n",
       " array([-0.96519077], dtype=float32),\n",
       " array([0.79844904], dtype=float32),\n",
       " array([0.54573065], dtype=float32),\n",
       " array([-0.99737084], dtype=float32),\n",
       " array([-0.83193266], dtype=float32),\n",
       " array([-0.9998795], dtype=float32),\n",
       " array([-0.4931119], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-0.99999845], dtype=float32),\n",
       " array([-0.99998915], dtype=float32),\n",
       " array([-0.99999994], dtype=float32),\n",
       " array([-0.999998], dtype=float32),\n",
       " array([0.36640558], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-0.99999994], dtype=float32),\n",
       " array([-0.9999997], dtype=float32),\n",
       " array([-0.99998564], dtype=float32),\n",
       " array([-0.52660525], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-0.8337203], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-0.9999999], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-0.99999946], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-0.99999994], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([-1.], dtype=float32)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_list[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6kKwRrTCdsbx",
    "outputId": "36e176e0-1a27-4527-8f4f-fb1c9098ca09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750,\n",
       " array(1.12773109),\n",
       " array(1.14453781),\n",
       " 0.9853157123162579,\n",
       " -4.818443765432343,\n",
       " 0.5021545554694419,\n",
       " -0.8632578419925802)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "stock = 1000\n",
    "action = -0.75\n",
    "t = 200\n",
    "price = np.array(X_test[t+1][0])\n",
    "next_price = np.array(X_test[t+2][0])\n",
    "punish_driver = 0.2\n",
    "reward_driver = 0.2\n",
    "interest_rate = 0.05\n",
    "\n",
    "amount = abs(math.floor( stock  * action))\n",
    "if action > 0:\n",
    "  reward_do = math.log10(float(next_price/price)) * amount\n",
    "else:\n",
    "  reward_do = math.log10(float(price/next_price)) * amount\n",
    "reward_leave = math.log10(1+float(interest_rate*30/365)) * (stock - amount) * price\n",
    "reward_function_sum = reward_do + reward_leave\n",
    "reward  = reward_function_sum * reward_driver if reward_function_sum >0  else reward_function_sum * punish_driver\n",
    "\n",
    "#leave_value = (stock - amount) * price\n",
    "#next_value = amount * next_price + leave_value\n",
    "#this_value = price * stock\n",
    "#reward_function = math.log10(float(this_value/next_value))\n",
    "#reward  = reward_function * reward_driver if reward_function >0  else reward_function * punish_driver\n",
    "\n",
    "amount,price,next_price,float(price/next_price),reward_do,reward_leave,reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi6e2-_pu05e"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW4d1YAMqif1"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # Defining the first Critic neural network\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, 1)\n",
    "        # Defining the second Critic neural network\n",
    "        self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_5 = nn.Linear(400, 300)\n",
    "        self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        # Forward-Propagation on the first Critic Neural Network\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        # Forward-Propagation on the second Critic Neural Network\n",
    "        x2 = F.relu(self.layer_4(xu))\n",
    "        x2 = F.relu(self.layer_5(x2))\n",
    "        x2 = self.layer_6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        return x1\n",
    "\n",
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "\n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "            state = torch.Tensor(batch_states).to(device)\n",
    "            next_state = torch.Tensor(batch_next_states).to(device)\n",
    "            action = torch.Tensor(batch_actions).to(device)\n",
    "            reward = torch.Tensor(batch_rewards).to(device)\n",
    "            done = torch.Tensor(batch_dones).to(device)\n",
    "\n",
    "            # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "            next_action = self.actor_target(next_state)\n",
    "\n",
    "            # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "\n",
    "            # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "\n",
    "            # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "\n",
    "            # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "            # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "            if it % policy_freq == 0:\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "            # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    # Making a save method to save a trained model\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "    while not done:\n",
    "        action = policy.select_action(np.array(obs))\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        avg_reward += reward\n",
    "    avg_reward /= eval_episodes\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "env_name = \"SALL_ENV\"\n",
    "seed = 0\n",
    "\n",
    "file_name = \"%s_%s(seed:%s)\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "import Enviorment_Setting\n",
    "\n",
    "\n",
    "eval_episodes = 10\n",
    "save_env_vid = True\n",
    "env = Enviorment_Setting.ETFenv(id = \"0050\")\n",
    "max_episode_steps = env._max_episode_steps\n",
    "\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = len(env.observation_space)\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "policy.load(file_name, '../Model_Repository/pytorch_models')\n",
    "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
